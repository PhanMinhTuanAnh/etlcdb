{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the `ETL2` dataset\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "Get and unzip the `ETL2` dataset as show below.\n",
    "\n",
    "```\n",
    "ETL2\n",
    "├── ETL2INFO\n",
    "├── ETL2_1\n",
    "├── ETL2_2\n",
    "├── ETL2_3\n",
    "├── ETL2_4\n",
    "└── ETL2_5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import struct\n",
    "import traceback\n",
    "import codecs\n",
    "\n",
    "# external dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import bitstring\n",
    "\n",
    "from PIL import Image\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl2_filepaths = [    \n",
    "    'ETL2/ETL2_1',\n",
    "    'ETL2/ETL2_2',\n",
    "    'ETL2/ETL2_3',\n",
    "    'ETL2/ETL2_4',\n",
    "    'ETL2/ETL2_5',\n",
    "]\n",
    "\n",
    "etl2_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset uses an uncommon encoding (`co59`) to encode the label so we need to prepare a map to convert them to unicode.\n",
    "\n",
    "The code below was taken from the official guide with the supplement `co59-utf8.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t56s = '0123456789[#@:>? ABCDEFGHI&.](<  JKLMNOPQR-$*);\\'|/STUVWXYZ ,%=\"!'\n",
    "\n",
    "def T56(c):\n",
    "    return t56s[c]\n",
    "\n",
    "with codecs.open('co59-utf8.txt', 'r', 'utf-8') as co59f:\n",
    "    co59t = co59f.read()\n",
    "\n",
    "co59l = co59t.split()\n",
    "CO59 = {}\n",
    "for c in co59l:\n",
    "    ch = c.split(':')\n",
    "    co = ch[1].split(',')\n",
    "    CO59[(int(co[0]), int(co[1]))] = ch[0]\n",
    "\n",
    "CO59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CO59.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look a sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_filepath = etl2_filepaths[0]\n",
    "sample_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stream = bitstring.ConstBitStream(filename=sample_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each records has the size of 3660 6-bit bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_LENGTH = 6 * 3660 # in bit(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change the skip value to inspect a different record\n",
    "skip = 0\n",
    "file_stream.pos = skip * RECORD_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the record as shown in the specification.\n",
    "\n",
    "http://etlcdb.db.aist.go.jp/specification-of-etl-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitstring_unpack_str = ','.join([\n",
    "    'int:36', # Serial Index - [0]\n",
    "    'uint:6', # Source ('A': Mincho Newspaper, 'B': Gothic Newspaper, 'C': Mincho Patent, 'D': Gothic Patent) - [1]\n",
    "    'pad:30', # padding bits - no index as they are skipped\n",
    "    '6*uint:6', # Class ('KANJI': kanji, 'EIJI': roman alphabets, 'HRKANA': hiragana, 'KTKANA': katakana, 'KIGO': special characters, 'SUUJI': numbers) - [2:8]\n",
    "    '6*uint:6', # Font ('MINCHO', 'GOTHIC') - [8:14]\n",
    "    'pad:24', # padding bits\n",
    "    '2*uint:6', # CO-59 Code - [14:16]\n",
    "    'pad:180', # padding bits\n",
    "    'bytes:2700', # 6-bit-depth image of 60 x 60 = 3600 pixels - [16]\n",
    "])\n",
    "\n",
    "record = file_stream.readlist(bitstring_unpack_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(record), len(record))\n",
    "record[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(record[0], T56(record[1]), ''.join(map(T56, record[2:8])), ''.join(map(T56, record[8:14])), CO59[tuple(record[14:16])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL2Record:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index: int, # the Serial Index in the record\n",
    "        source: str, # the source material that the record has been scanned from\n",
    "        character_type: str, # enum type: 'KANJI', 'EIJI', 'HRKANA', 'KTKANA', 'KIGO', 'SUUJI'\n",
    "        font: str, # e.g. \"('MINCHO', 'GOTHIC')\"\n",
    "        unicode_char: str, # e.g. あ\n",
    "        image: bytes, # PNG encoded image\n",
    "    ):\n",
    "        self.index = index\n",
    "        self.source = source\n",
    "        self.character_type = character_type\n",
    "        self.font = font\n",
    "        self.unicode_char = unicode_char\n",
    "        self.image = image\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the images are 6-bit depth 60x60 pixels images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 60\n",
    "IMG_HEIGHT = 60\n",
    "\n",
    "pil_image = Image.frombytes('F', (IMG_WIDTH, IMG_HEIGHT), record[16], 'bit', 6)\n",
    "\n",
    "np_img = np.array(pil_image)\n",
    "\n",
    "plt.imshow(np_img)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert image to grayscale `[0-255]` range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image = pil_image.convert('L')\n",
    "np_img = np.array(pil_image)\n",
    "\n",
    "plt.imshow(np_img)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = io.BytesIO()\n",
    "pil_image.save(buffer, format='PNG')\n",
    "png_encoded_image = buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image = Image.frombytes('F', (IMG_WIDTH, IMG_HEIGHT), record[16], 'bit', 6)\n",
    "pil_image = pil_image.convert('L')\n",
    "\n",
    "buffer = io.BytesIO()\n",
    "pil_image.save(buffer, format='PNG')\n",
    "png_encoded_image = buffer.getvalue()\n",
    "\n",
    "etl2_record = ETL2Record(\n",
    "    index=record[0],\n",
    "    source=T56(record[1]),\n",
    "    character_type=''.join(map(T56, record[2:8])),\n",
    "    font=''.join(map(T56, record[8:14])),\n",
    "    unicode_char=CO59[tuple(record[14:16])],\n",
    "    image=png_encoded_image,\n",
    ")\n",
    "\n",
    "etl2_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all for inspecting a single record in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`XFormat` is my custom data serialization format. I created this format because I don't want to use `JSON` or `TFRecord` (`protobuf`) for various limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XFormat:\n",
    "    INT_SIZE = 4\n",
    "    BYTE_ORDER = 'little'\n",
    "    EXTENSION = '.xformat'\n",
    "    ENCODING = 'utf-8'\n",
    "\n",
    "    DATA_TYPE_BYTES = 0\n",
    "    DATA_TYPE_INT = 1\n",
    "    DATA_TYPE_UTF8_STRING = 2\n",
    "    DATA_TYPE_LIST = 3\n",
    "    DATA_TYPE_DICT = 4\n",
    "\n",
    "    @classmethod\n",
    "    def serialize_string(cls, s: str) -> bytes:\n",
    "        record_data = s.encode(encoding=cls.ENCODING)\n",
    "        return record_data\n",
    "\n",
    "    @classmethod\n",
    "    def deserialize_string(cls, bs: bytes) -> str:\n",
    "        return bs.decode(encoding=cls.ENCODING)\n",
    "\n",
    "    @classmethod\n",
    "    def serialize_int(cls, n: int) -> bytes:\n",
    "        record_data = n.to_bytes(\n",
    "            length=cls.INT_SIZE,\n",
    "            byteorder=cls.BYTE_ORDER,\n",
    "            signed=True,\n",
    "        )\n",
    "\n",
    "        return record_data\n",
    "\n",
    "    @classmethod\n",
    "    def deserialize_int(cls, bs: bytes) -> int:\n",
    "        return int.from_bytes(bs, byteorder=cls.BYTE_ORDER, signed=True)\n",
    "\n",
    "    @classmethod\n",
    "    def serialize_obj(cls, obj) -> (bytes, bytes):\n",
    "        obj_type = type(obj)\n",
    "        if obj_type == int:\n",
    "            return bytes([cls.DATA_TYPE_INT]), cls.serialize_int(obj)\n",
    "        elif obj_type == str:\n",
    "            return bytes([cls.DATA_TYPE_UTF8_STRING]), cls.serialize_string(obj)\n",
    "        elif obj_type == bytes:\n",
    "            return bytes([cls.DATA_TYPE_BYTES]), obj\n",
    "        elif obj_type == list:\n",
    "            buffer = io.BytesIO()\n",
    "\n",
    "            for value in obj:\n",
    "                datatype, encoded_value = cls.serialize_obj(value)\n",
    "                buffer.write(datatype)\n",
    "                buffer.write(cls.serialize_int(len(encoded_value)))\n",
    "                buffer.write(encoded_value)\n",
    "\n",
    "            return bytes([cls.DATA_TYPE_LIST]), buffer.getvalue()\n",
    "        elif obj_type == dict:\n",
    "            buffer = io.BytesIO()\n",
    "\n",
    "            for key in obj:\n",
    "                datatype, encoded_key = cls.serialize_obj(key)\n",
    "                buffer.write(datatype)\n",
    "                buffer.write(cls.serialize_int(len(encoded_key)))\n",
    "                buffer.write(encoded_key)\n",
    "\n",
    "                datatype, encoded_value = cls.serialize_obj(obj[key])\n",
    "                buffer.write(datatype)\n",
    "                buffer.write(cls.serialize_int(len(encoded_value)))\n",
    "                buffer.write(encoded_value)\n",
    "\n",
    "            return bytes([cls.DATA_TYPE_DICT]), buffer.getvalue()\n",
    "        else:\n",
    "            raise Exception(f'Unsupported type {obj_type}!')\n",
    "            return 0\n",
    "\n",
    "    @classmethod\n",
    "    def deserialze_obj(cls, bs: bytes, datatype: int):\n",
    "        if datatype == cls.DATA_TYPE_BYTES:\n",
    "            return bs\n",
    "        elif datatype == cls.DATA_TYPE_INT:\n",
    "            return cls.deserialize_int(bs)\n",
    "        elif datatype == cls.DATA_TYPE_UTF8_STRING:\n",
    "            return cls.deserialize_string(bs)\n",
    "        elif datatype == cls.DATA_TYPE_LIST:\n",
    "            retval = []\n",
    "            buffer = io.BytesIO(bs)\n",
    "            pos = 0\n",
    "            bs_len = len(bs)\n",
    "\n",
    "            while pos < bs_len:\n",
    "                value_datatype = bs[pos]\n",
    "                pos += 1\n",
    "\n",
    "                if(pos + cls.INT_SIZE) > bs_len:\n",
    "                    raise Exception(f'Broken serialized data!')\n",
    "                value_byte_count = cls.deserialize_int(bs[pos:pos+cls.INT_SIZE])  # noqa\n",
    "                pos += cls.INT_SIZE\n",
    "\n",
    "                if(pos + value_byte_count) > bs_len:\n",
    "                    raise Exception(f'Broken serialized data!')\n",
    "                value = cls.deserialze_obj(bs[pos:pos+value_byte_count], value_datatype)  # noqa\n",
    "                retval.append(value)\n",
    "                pos += value_byte_count\n",
    "\n",
    "            return retval\n",
    "        elif datatype == cls.DATA_TYPE_DICT:\n",
    "            retval = {}\n",
    "            buffer = io.BytesIO(bs)\n",
    "            pos = 0\n",
    "            bs_len = len(bs)\n",
    "\n",
    "            while pos < bs_len:\n",
    "                key_datatype = bs[pos]\n",
    "                pos += 1\n",
    "\n",
    "                if (pos + cls.INT_SIZE) > bs_len:\n",
    "                    raise Exception(f'Broken serialized data!')\n",
    "                key_byte_count = cls.deserialize_int(bs[pos:pos+cls.INT_SIZE])\n",
    "                pos += cls.INT_SIZE\n",
    "\n",
    "                if(pos + key_byte_count) > bs_len:\n",
    "                    raise Exception(f'Broken serialized data!')\n",
    "                key = cls.deserialze_obj(bs[pos:pos+key_byte_count], key_datatype)  # noqa\n",
    "                pos += key_byte_count\n",
    "\n",
    "                value_datatype = bs[pos]\n",
    "                pos += 1\n",
    "\n",
    "                if(pos + cls.INT_SIZE) > bs_len:\n",
    "                    raise Exception(f'Broken serialized data!')\n",
    "                value_byte_count = cls.deserialize_int(bs[pos:pos+cls.INT_SIZE])  # noqa\n",
    "                pos += cls.INT_SIZE\n",
    "\n",
    "                if(pos + value_byte_count) > bs_len:\n",
    "                    raise Exception(f'Broken serialized data!')\n",
    "                value = cls.deserialze_obj(bs[pos:pos+value_byte_count], value_datatype)  # noqa\n",
    "                pos += value_byte_count\n",
    "\n",
    "                retval[key] = value\n",
    "\n",
    "            return retval\n",
    "        else:\n",
    "            raise Exception(f'Unsupported data type {datatype}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_metadata = []\n",
    "etl2_serialized_dataset_filepath = f'etl2{XFormat.EXTENSION}'\n",
    "\n",
    "with open(etl2_serialized_dataset_filepath, mode='wb') as out_stream:\n",
    "    pbar = tqdm(etl2_filepaths)\n",
    "    for filename in pbar:\n",
    "\n",
    "        file_stream = bitstring.ConstBitStream(filename=filename)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                record = file_stream.readlist(bitstring_unpack_str)\n",
    "            except:\n",
    "                # TODO properly check for end of file\n",
    "                # print(record)\n",
    "                # traceback.print_exc()\n",
    "                break\n",
    "\n",
    "            pil_image = Image.frombytes('F', (IMG_WIDTH, IMG_HEIGHT), record[16], 'bit', 6)\n",
    "            pil_image = pil_image.convert('L')\n",
    "\n",
    "            buffer = io.BytesIO()\n",
    "            pil_image.save(buffer, format='PNG')\n",
    "            png_encoded_image = buffer.getvalue()\n",
    "\n",
    "            etl2_record = ETL2Record(\n",
    "                index=record[0],\n",
    "                source=T56(record[1]),\n",
    "                character_type=''.join(map(T56, record[2:8])),\n",
    "                font=''.join(map(T56, record[8:14])),\n",
    "                unicode_char=CO59[tuple(record[14:16])],\n",
    "                image=png_encoded_image,\n",
    "            )\n",
    "\n",
    "            record_datatype, serialized_record = XFormat.serialize_obj(etl2_record.__dict__)\n",
    "            record_byte_count = len(serialized_record)\n",
    "            record_seek_start = out_stream.tell()\n",
    "\n",
    "            out_stream.write(record_datatype)\n",
    "            out_stream.write(XFormat.serialize_int(record_byte_count))\n",
    "            out_stream.write(serialized_record)\n",
    "\n",
    "            record_seek_end = out_stream.tell()\n",
    "            \n",
    "            record_metadata = {\n",
    "                'index': etl2_record.index,\n",
    "                'source': etl2_record.source,\n",
    "                'character_type': etl2_record.character_type,\n",
    "                'font': etl2_record.font,\n",
    "                'unicode_char': etl2_record.unicode_char,\n",
    "                'seek_start': record_seek_start,\n",
    "                'seek_end': record_seek_end,\n",
    "            }\n",
    "            \n",
    "            records_metadata.append(record_metadata)\n",
    "            \n",
    "            pbar.set_description(f'{filename} - {etl2_record.index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('etl2-metadata.json', mode='w', encoding='utf-8') as out_stream:\n",
    "    json.dump(records_metadata, out_stream, ensure_ascii=False, indent='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
